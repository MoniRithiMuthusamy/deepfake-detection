{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggs2aoROxR0o",
        "outputId": "2cf6731e-9b8d-4b52-f13e-6fa6eeb60744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset already extracted at /content/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 4.2004, Accuracy: 46.00%\n",
            "Epoch [2/20], Loss: 3.9918, Accuracy: 54.00%\n",
            "Epoch [3/20], Loss: 3.3827, Accuracy: 79.00%\n",
            "Epoch [4/20], Loss: 0.8274, Accuracy: 89.00%\n",
            "Epoch [5/20], Loss: 0.7948, Accuracy: 91.00%\n",
            "Epoch [6/20], Loss: 0.7400, Accuracy: 92.00%\n",
            "Epoch [7/20], Loss: 0.6159, Accuracy: 94.00%\n",
            "Epoch [8/20], Loss: 0.6402, Accuracy: 97.00%\n",
            "Epoch [9/20], Loss: 0.8129, Accuracy: 95.00%\n",
            "Epoch [10/20], Loss: 0.2900, Accuracy: 96.00%\n",
            "Epoch [11/20], Loss: 0.2313, Accuracy: 98.00%\n",
            "Epoch [12/20], Loss: 0.0779, Accuracy: 100.00%\n",
            "Epoch [13/20], Loss: 0.3254, Accuracy: 96.00%\n",
            "Epoch [14/20], Loss: 0.3430, Accuracy: 97.00%\n",
            "Epoch [15/20], Loss: 0.7164, Accuracy: 97.00%\n",
            "Epoch [16/20], Loss: 0.6958, Accuracy: 98.00%\n",
            "Epoch [17/20], Loss: 0.4856, Accuracy: 95.00%\n",
            "Epoch [18/20], Loss: 0.3044, Accuracy: 97.00%\n",
            "Epoch [19/20], Loss: 0.1998, Accuracy: 99.00%\n",
            "Epoch [20/20], Loss: 0.8179, Accuracy: 97.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBZfoVL1ySEL",
        "outputId": "e6b2c13f-eb6c-4936-89b7-9e2dcf2e8af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6dd2da831e31>:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 52.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C7hfnXz4_Os",
        "outputId": "61224f3e-4030-4f0c-c218-ba568775ecb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset already extracted at /content/Dataset\n",
            "Epoch [1/20], Loss: 5.0230, Accuracy: 46.00%\n",
            "Epoch [2/20], Loss: 2.3059, Accuracy: 63.00%\n",
            "Epoch [3/20], Loss: 1.6708, Accuracy: 81.00%\n",
            "Epoch [4/20], Loss: 1.1032, Accuracy: 87.00%\n",
            "Epoch [5/20], Loss: 0.6454, Accuracy: 92.00%\n",
            "Epoch [6/20], Loss: 0.9049, Accuracy: 95.00%\n",
            "Epoch [7/20], Loss: 0.3585, Accuracy: 95.00%\n",
            "Epoch [8/20], Loss: 0.3598, Accuracy: 94.00%\n",
            "Epoch [9/20], Loss: 1.1648, Accuracy: 95.00%\n",
            "Epoch [10/20], Loss: 1.9244, Accuracy: 98.00%\n",
            "Epoch [11/20], Loss: 0.6116, Accuracy: 99.00%\n",
            "Epoch [12/20], Loss: 0.4886, Accuracy: 95.00%\n",
            "Epoch [13/20], Loss: 0.2577, Accuracy: 97.00%\n",
            "Epoch [14/20], Loss: 0.4149, Accuracy: 98.00%\n",
            "Epoch [15/20], Loss: 0.2724, Accuracy: 97.00%\n",
            "Epoch [16/20], Loss: 0.2229, Accuracy: 97.00%\n",
            "Epoch [17/20], Loss: 0.2217, Accuracy: 98.00%\n",
            "Epoch [18/20], Loss: 0.3256, Accuracy: 97.00%\n",
            "Epoch [19/20], Loss: 0.8178, Accuracy: 97.00%\n",
            "Epoch [20/20], Loss: 0.8824, Accuracy: 97.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL1a-ppddEp7",
        "outputId": "215e01bc-61d8-49ae-ff3c-1fc3e429e3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-6dd2da831e31>:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 40.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j7uWoxQhl9-",
        "outputId": "55749afc-a490-4c60-e72d-e2fc00426498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset unzipped to /content/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 122MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 5.6736, Accuracy: 47.00%\n",
            "Epoch [2/20], Loss: 2.1181, Accuracy: 70.00%\n",
            "Epoch [3/20], Loss: 1.9107, Accuracy: 83.00%\n",
            "Epoch [4/20], Loss: 0.7749, Accuracy: 90.00%\n",
            "Epoch [5/20], Loss: 0.8125, Accuracy: 88.00%\n",
            "Epoch [6/20], Loss: 0.4200, Accuracy: 95.00%\n",
            "Epoch [7/20], Loss: 0.4434, Accuracy: 96.00%\n",
            "Epoch [8/20], Loss: 0.3507, Accuracy: 95.00%\n",
            "Epoch [9/20], Loss: 0.1244, Accuracy: 98.00%\n",
            "Epoch [10/20], Loss: 0.0702, Accuracy: 100.00%\n",
            "Epoch [11/20], Loss: 0.2799, Accuracy: 96.00%\n",
            "Epoch [12/20], Loss: 0.0795, Accuracy: 99.00%\n",
            "Epoch [13/20], Loss: 0.9246, Accuracy: 97.00%\n",
            "Epoch [14/20], Loss: 0.0867, Accuracy: 99.00%\n",
            "Epoch [15/20], Loss: 0.7494, Accuracy: 95.00%\n",
            "Epoch [16/20], Loss: 0.4213, Accuracy: 93.00%\n",
            "Epoch [17/20], Loss: 0.6749, Accuracy: 94.00%\n",
            "Epoch [18/20], Loss: 2.2996, Accuracy: 97.00%\n",
            "Epoch [19/20], Loss: 0.1280, Accuracy: 100.00%\n",
            "Epoch [20/20], Loss: 0.0666, Accuracy: 100.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCHER_Rw7KBX",
        "outputId": "2f7873ad-9fd9-4bca-88da-17326e5cb1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 52.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbQiCcHE_yp1",
        "outputId": "9c6df31d-9541-4022-dd1d-2ca40ea2a47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset already extracted at /content/Dataset\n",
            "Epoch [1/20], Loss: 5.1228, Accuracy: 45.00%\n",
            "Epoch [2/20], Loss: 2.5593, Accuracy: 66.00%\n",
            "Epoch [3/20], Loss: 1.3413, Accuracy: 92.00%\n",
            "Epoch [4/20], Loss: 1.0495, Accuracy: 88.00%\n",
            "Epoch [5/20], Loss: 1.7740, Accuracy: 91.00%\n",
            "Epoch [6/20], Loss: 0.9721, Accuracy: 93.00%\n",
            "Epoch [7/20], Loss: 0.6521, Accuracy: 93.00%\n",
            "Epoch [8/20], Loss: 1.0574, Accuracy: 96.00%\n",
            "Epoch [9/20], Loss: 0.3041, Accuracy: 97.00%\n",
            "Epoch [10/20], Loss: 0.3734, Accuracy: 96.00%\n",
            "Epoch [11/20], Loss: 0.4158, Accuracy: 97.00%\n",
            "Epoch [12/20], Loss: 0.0883, Accuracy: 99.00%\n",
            "Epoch [13/20], Loss: 0.1843, Accuracy: 97.00%\n",
            "Epoch [14/20], Loss: 0.3190, Accuracy: 98.00%\n",
            "Epoch [15/20], Loss: 0.2172, Accuracy: 99.00%\n",
            "Epoch [16/20], Loss: 0.7477, Accuracy: 97.00%\n",
            "Epoch [17/20], Loss: 0.1007, Accuracy: 99.00%\n",
            "Epoch [18/20], Loss: 0.0331, Accuracy: 100.00%\n",
            "Epoch [19/20], Loss: 0.4919, Accuracy: 97.00%\n",
            "Epoch [20/20], Loss: 0.0677, Accuracy: 100.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZRC61W5_-0v",
        "outputId": "d26c9a7e-5590-44da-e66e-3c7268a0634e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 49.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMsrBkNxE6qv",
        "outputId": "9a81930e-1133-473c-a2a7-bb6ea4685df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset already extracted at /content/Dataset\n",
            "Epoch [1/20], Loss: 3.2772, Accuracy: 47.00%\n",
            "Epoch [2/20], Loss: 2.2386, Accuracy: 70.00%\n",
            "Epoch [3/20], Loss: 1.4540, Accuracy: 88.00%\n",
            "Epoch [4/20], Loss: 1.6060, Accuracy: 78.00%\n",
            "Epoch [5/20], Loss: 1.7391, Accuracy: 92.00%\n",
            "Epoch [6/20], Loss: 1.4805, Accuracy: 92.00%\n",
            "Epoch [7/20], Loss: 0.8450, Accuracy: 94.00%\n",
            "Epoch [8/20], Loss: 1.7041, Accuracy: 86.00%\n",
            "Epoch [9/20], Loss: 0.3731, Accuracy: 93.00%\n",
            "Epoch [10/20], Loss: 0.5387, Accuracy: 95.00%\n",
            "Epoch [11/20], Loss: 0.5266, Accuracy: 96.00%\n",
            "Epoch [12/20], Loss: 0.6522, Accuracy: 96.00%\n",
            "Epoch [13/20], Loss: 1.4063, Accuracy: 97.00%\n",
            "Epoch [14/20], Loss: 0.8130, Accuracy: 94.00%\n",
            "Epoch [15/20], Loss: 0.8431, Accuracy: 96.00%\n",
            "Epoch [16/20], Loss: 0.2458, Accuracy: 99.00%\n",
            "Epoch [17/20], Loss: 0.2106, Accuracy: 97.00%\n",
            "Epoch [18/20], Loss: 0.6533, Accuracy: 96.00%\n",
            "Epoch [19/20], Loss: 0.1953, Accuracy: 99.00%\n",
            "Epoch [20/20], Loss: 0.0903, Accuracy: 99.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOHr1aQyFFsL",
        "outputId": "39b2fd17-50fb-465c-b44b-18031e76447e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 41.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88koU5KsJb1m",
        "outputId": "1a4239d7-f6ec-4b71-b1ea-b49e63be6b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset unzipped to /content/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 149MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 5.1791, Accuracy: 50.00%\n",
            "Epoch [2/20], Loss: 3.5516, Accuracy: 72.00%\n",
            "Epoch [3/20], Loss: 1.2588, Accuracy: 86.00%\n",
            "Epoch [4/20], Loss: 1.7239, Accuracy: 90.00%\n",
            "Epoch [5/20], Loss: 0.8750, Accuracy: 90.00%\n",
            "Epoch [6/20], Loss: 0.7122, Accuracy: 92.00%\n",
            "Epoch [7/20], Loss: 0.7061, Accuracy: 86.00%\n",
            "Epoch [8/20], Loss: 0.4879, Accuracy: 92.00%\n",
            "Epoch [9/20], Loss: 0.2014, Accuracy: 97.00%\n",
            "Epoch [10/20], Loss: 0.2498, Accuracy: 97.00%\n",
            "Epoch [11/20], Loss: 0.1330, Accuracy: 99.00%\n",
            "Epoch [12/20], Loss: 0.3577, Accuracy: 98.00%\n",
            "Epoch [13/20], Loss: 0.1025, Accuracy: 99.00%\n",
            "Epoch [14/20], Loss: 0.1677, Accuracy: 98.00%\n",
            "Epoch [15/20], Loss: 0.1076, Accuracy: 99.00%\n",
            "Epoch [16/20], Loss: 0.2444, Accuracy: 96.00%\n",
            "Epoch [17/20], Loss: 0.0911, Accuracy: 99.00%\n",
            "Epoch [18/20], Loss: 0.0792, Accuracy: 99.00%\n",
            "Epoch [19/20], Loss: 0.0259, Accuracy: 100.00%\n",
            "Epoch [20/20], Loss: 0.0503, Accuracy: 100.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2fHybQIVZ4y",
        "outputId": "46c494e9-660d-476a-8f29-f012b465fff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 48.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\"‚úÖ Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\"‚úÖ Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RizUDjJEaLSt",
        "outputId": "ede3da55-088a-452b-c013-e6fae0e00e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset unzipped to /content/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 87.2MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 5.6522, Accuracy: 52.00%\n",
            "Epoch [2/20], Loss: 2.0648, Accuracy: 72.00%\n",
            "Epoch [3/20], Loss: 1.5949, Accuracy: 83.00%\n",
            "Epoch [4/20], Loss: 2.0131, Accuracy: 78.00%\n",
            "Epoch [5/20], Loss: 1.2279, Accuracy: 80.00%\n",
            "Epoch [6/20], Loss: 0.8143, Accuracy: 92.00%\n",
            "Epoch [7/20], Loss: 1.9641, Accuracy: 94.00%\n",
            "Epoch [8/20], Loss: 0.7873, Accuracy: 93.00%\n",
            "Epoch [9/20], Loss: 0.3379, Accuracy: 95.00%\n",
            "Epoch [10/20], Loss: 0.3483, Accuracy: 98.00%\n",
            "Epoch [11/20], Loss: 1.3683, Accuracy: 95.00%\n",
            "Epoch [12/20], Loss: 2.2900, Accuracy: 97.00%\n",
            "Epoch [13/20], Loss: 1.1145, Accuracy: 93.00%\n",
            "Epoch [14/20], Loss: 1.6160, Accuracy: 96.00%\n",
            "Epoch [15/20], Loss: 0.5955, Accuracy: 97.00%\n",
            "Epoch [16/20], Loss: 0.4501, Accuracy: 95.00%\n",
            "Epoch [17/20], Loss: 0.2039, Accuracy: 100.00%\n",
            "Epoch [18/20], Loss: 0.2692, Accuracy: 99.00%\n",
            "Epoch [19/20], Loss: 0.5547, Accuracy: 97.00%\n",
            "Epoch [20/20], Loss: 0.1290, Accuracy: 98.00%\n",
            "‚úÖ Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Test.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlwyoHDxAlBG",
        "outputId": "bedbe160-5afc-4d2c-92b5-14343b374297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 300 images in the dataset.\n",
            "‚úÖ Model Test Accuracy: 46.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Unzip dataset if needed\n",
        "dataset_zip_path = r\"/content/Dataset.zip\"\n",
        "extracted_dir = r\"/content/Dataset\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "    with ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "        print(f\" Dataset unzipped to {extracted_dir}\")\n",
        "else:\n",
        "    print(f\" Dataset already extracted at {extracted_dir}\")\n",
        "\n",
        "# Step 2: Path to training dataset\n",
        "train_dir = os.path.join(extracted_dir, 'Dataset', 'Test')\n",
        "\n",
        "# Feature extractor using DenseNet121\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_dim=1024):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(1024, output_dim)  # Ensure 1024 output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)  # Ensures correct output size\n",
        "\n",
        "# CrossViT Transformer block\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=3):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# Full DeepFake Detector Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        return self.classifier(encoded_features[:, 0, :])\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Step 3: Load dataset with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    scheduler.step(total_loss)  # Adjust learning rate if loss stagnates\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_model.pth\")\n",
        "print(\" Model training complete! Saved as deepfake_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdj8vxJiG1LZ",
        "outputId": "d7f60d53-c072-4409-fec3-bd76f06c9a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset unzipped to /content/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 123MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 4.3142, Accuracy: 52.00%\n",
            "Epoch [2/20], Loss: 4.6982, Accuracy: 54.00%\n",
            "Epoch [3/20], Loss: 1.6568, Accuracy: 77.00%\n",
            "Epoch [4/20], Loss: 1.4861, Accuracy: 77.00%\n",
            "Epoch [5/20], Loss: 0.6410, Accuracy: 95.00%\n",
            "Epoch [6/20], Loss: 0.6949, Accuracy: 89.00%\n",
            "Epoch [7/20], Loss: 1.9593, Accuracy: 93.00%\n",
            "Epoch [8/20], Loss: 3.3368, Accuracy: 94.00%\n",
            "Epoch [9/20], Loss: 0.5018, Accuracy: 96.00%\n",
            "Epoch [10/20], Loss: 0.3327, Accuracy: 97.00%\n",
            "Epoch [11/20], Loss: 1.1894, Accuracy: 88.00%\n",
            "Epoch [12/20], Loss: 0.3743, Accuracy: 96.00%\n",
            "Epoch [13/20], Loss: 0.2682, Accuracy: 97.00%\n",
            "Epoch [14/20], Loss: 0.4491, Accuracy: 96.00%\n",
            "Epoch [15/20], Loss: 0.9188, Accuracy: 98.00%\n",
            "Epoch [16/20], Loss: 0.8099, Accuracy: 98.00%\n",
            "Epoch [17/20], Loss: 0.6035, Accuracy: 96.00%\n",
            "Epoch [18/20], Loss: 0.1353, Accuracy: 100.00%\n",
            "Epoch [19/20], Loss: 0.4153, Accuracy: 98.00%\n",
            "Epoch [20/20], Loss: 0.1801, Accuracy: 100.00%\n",
            " Model training complete! Saved as deepfake_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Directory to save feature images\n",
        "feature_output_dir = \"/content/densenet_features1\"\n",
        "os.makedirs(feature_output_dir, exist_ok=True)\n",
        "\n",
        "# Path to test dataset (modify if needed)\n",
        "test_dir = \"/content/Dataset_f/Test\"\n",
        "\n",
        "# Define transformations\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load trained model (ensure the model is defined and loaded properly)\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Model evaluation with F1-score calculation and feature extraction\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        features = model.dense_feature_extractor(images)  # Extract DenseNet features\n",
        "\n",
        "        # Save each feature map as an image\n",
        "        for i in range(features.shape[0]):  # Iterate over batch\n",
        "            feature_map = features[i].cpu().numpy()  # Convert to NumPy\n",
        "            feature_map = np.mean(feature_map, axis=0)  # Average over channels\n",
        "\n",
        "            # Normalize to 0-1 range\n",
        "            feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)\n",
        "\n",
        "            # Save feature image\n",
        "            plt.imsave(os.path.join(feature_output_dir, f\"feature_{batch_idx}_{i}.png\"), feature_map, cmap='viridis')\n",
        "\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Store predictions and labels for F1-score calculation\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')  # Weighted F1-score\n",
        "\n",
        "    print(f\" Model Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\" Model F1-Score: {f1:.4f}\")\n",
        "    print(f\"Feature maps saved in: {feature_output_dir}\")\n",
        "else:\n",
        "    print(\" No data to evaluate.\")\n"
      ],
      "metadata": {
        "id": "jMAN21c1H4Q-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "88d303c2-c87e-468b-f648-0010a0084b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Dataset_f/Test'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-90d7005628ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Load test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Dataset_f/Test'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Dataset_f.zip\"\n",
        "extract_to = \"/content/Dataset_f\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Model evaluation\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Directory to save feature images\n",
        "feature_output_dir = \"/content/densenet_features1\"\n",
        "os.makedirs(feature_output_dir, exist_ok=True)\n",
        "\n",
        "# Path to test dataset (modify if needed)\n",
        "#test_dir = \"/content/Dataset_f/Test\"\n",
        "test_dir=extracted_test_dir\n",
        "# Define transformations\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load trained model (ensure the model is defined and loaded properly)\n",
        "model = torch.load(\"deepfake_model.pth\", map_location=device)\n",
        "model.eval()\n",
        "\n",
        "# Model evaluation with F1-score calculation and feature extraction\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        features = model.dense_feature_extractor(images)  # Extract DenseNet features\n",
        "\n",
        "        # Save each feature map as an image\n",
        "        for i in range(features.shape[0]):  # Iterate over batch\n",
        "            feature_map = features[i].cpu().numpy()  # Convert to NumPy\n",
        "            feature_map = np.mean(feature_map, axis=0)  # Average over channels\n",
        "\n",
        "            # Normalize to 0-1 range\n",
        "            feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)\n",
        "\n",
        "            # Save feature image\n",
        "            plt.imsave(os.path.join(feature_output_dir, f\"feature_{batch_idx}_{i}.png\"), feature_map, cmap='viridis')\n",
        "\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Store predictions and labels for F1-score calculation\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')  # Weighted F1-score\n",
        "\n",
        "    print(f\" Model Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\" Model F1-Score: {f1:.4f}\")\n",
        "    print(f\"Feature maps saved in: {feature_output_dir}\")\n",
        "else:\n",
        "    print(\" No data to evaluate.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "NrzDyQuIsdpP",
        "outputId": "f2363f1c-5446-4d38-cdd9-6bba3c3df6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 900 images in the dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-95c4af77a1a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# Load trained model (ensure the model is defined and loaded properly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deepfake_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Model evaluation with F1-score calculation and feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Dataset_f.zip\"\n",
        "extract_to = \"/content/Dataset_f\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"\\U0001F504 Extracting Dataset_f.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset class for loading images\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "    print(f\"Missing keys: {missing_keys}\")\n",
        "    print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "    model.eval()\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ODHpcUaxRrQ",
        "outputId": "b851f341-dbc6-43cc-ced5-b40b43ea3721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting Dataset_f.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "Missing keys: []\n",
            "Unexpected keys: ['dense_feature_extractor.fc.weight', 'dense_feature_extractor.fc.bias', 'cross_vit.transformer_encoder.layers.2.self_attn.in_proj_weight', 'cross_vit.transformer_encoder.layers.2.self_attn.in_proj_bias', 'cross_vit.transformer_encoder.layers.2.self_attn.out_proj.weight', 'cross_vit.transformer_encoder.layers.2.self_attn.out_proj.bias', 'cross_vit.transformer_encoder.layers.2.linear1.weight', 'cross_vit.transformer_encoder.layers.2.linear1.bias', 'cross_vit.transformer_encoder.layers.2.linear2.weight', 'cross_vit.transformer_encoder.layers.2.linear2.bias', 'cross_vit.transformer_encoder.layers.2.norm1.weight', 'cross_vit.transformer_encoder.layers.2.norm1.bias', 'cross_vit.transformer_encoder.layers.2.norm2.weight', 'cross_vit.transformer_encoder.layers.2.norm2.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMFqjzez0Aqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gig7_E3p0BHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Ensure dataset is extracted before use\n",
        "zip_path = \"/content/Dataset_f.zip\"\n",
        "extract_to = \"/content/Test\"\n",
        "\n",
        "if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "    print(\"üîÑ Extracting testdata.zip...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete!\")\n",
        "\n",
        "# Update dataset directory after extraction\n",
        "extracted_test_dir = extract_to\n",
        "\n",
        "# Check dataset presence\n",
        "if not os.path.exists(extracted_test_dir) or not os.listdir(extracted_test_dir):\n",
        "    print(f\"‚ùå No images found in {extracted_test_dir}. Please verify the dataset.\")\n",
        "    exit()\n",
        "\n",
        "# Dataset class for loading images from subdirectories\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Scan subdirectories\n",
        "        for subdir, _, files in os.walk(root_dir):\n",
        "            for img_name in files:\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(subdir, img_name)\n",
        "\n",
        "                    # Assign label based on subdirectory or filename\n",
        "                    label = 0 if \"real\" in subdir.lower() or \"real\" in img_name.lower() else 1\n",
        "                    self.data.append((img_path, label))\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.data)*3} images in the dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Feature extractor using DenseNet\n",
        "class DenseNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetFeatureExtractor, self).__init__()\n",
        "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
        "        self.features = densenet.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 1024)\n",
        "        return x\n",
        "\n",
        "# Transformer-based classifier (CrossViT)\n",
        "class CrossViT(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=512, num_heads=8, num_layers=2):\n",
        "        super(CrossViT, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.transformer_encoder(tokens)\n",
        "\n",
        "# DeepFake Detection Model\n",
        "class DeepFakeDetector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepFakeDetector, self).__init__()\n",
        "        self.dense_feature_extractor = DenseNetFeatureExtractor()\n",
        "        self.cross_vit = CrossViT(input_dim=1024)\n",
        "        self.classifier = nn.Linear(1024, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.dense_feature_extractor(x)\n",
        "        features = features.unsqueeze(1)  # Shape: (batch_size, 1, 1024)\n",
        "        encoded_features = self.cross_vit(features)\n",
        "        output = self.classifier(encoded_features[:, 0, :])\n",
        "        return output\n",
        "\n",
        "# Load model and weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepFakeDetector().to(device)\n",
        "\n",
        "model_path = \"/content/deepfake_model.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint, strict=False)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"‚ùå Model file not found at {model_path}\")\n",
        "    exit()\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = TestDataset(root_dir=extracted_test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "'''\n",
        "# Model evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "if total > 0:\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\n",
        "else:\n",
        "    print(\"‚ùå No data to evaluate.\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "xRyd4E7v0BjL",
        "outputId": "0f439b0f-ed5d-49ea-f786-012efbc6f117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extracting testdata.zip...\n",
            "‚úÖ Extraction complete!\n",
            "‚úÖ Model loaded successfully!\n",
            "‚úÖ Found 900 images in the dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Model evaluation\\ncorrect = 0\\ntotal = 0\\n\\nwith torch.no_grad():\\n    for images, labels in test_loader:\\n        images, labels = images.to(device), labels.to(device)\\n        outputs = model(images)\\n        predicted = torch.argmax(outputs, dim=1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\nif total > 0:\\n    accuracy = 100 * correct / total\\n    print(f\"‚úÖ Model Test Accuracy: {accuracy:.2f}%\")\\nelse:\\n    print(\"‚ùå No data to evaluate.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWISp0gM0vv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}